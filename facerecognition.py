# -*- coding: utf-8 -*-
"""Final_Machine_Learning_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_nYmFfViqvx_HRTbj3lXfN2y02CD71ha

# **Final Machine Learning Project :**


---
"""

! pip install faiss-cpu
! pip install gradio
! pip install facenet-pytorch

import numpy as np
import matplotlib.pyplot as plt
import cv2 as cv
from PIL import Image

from sklearn.datasets import fetch_lfw_people
from facenet_pytorch import InceptionResnetV1, MTCNN
import torch
import faiss
import gradio as gr

"""- **Loading the dataset :**

---


"""

data = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
faces = data.images
labels = data.target_names[data.target]

print(f"Number of faces : {len(faces)}")
print(f"Image shape : {faces[0].shape}")
print(f"Labels : {labels}")

plt.figure(figsize=(13,5))
for i in range(1,11):
    plt.subplot(2,5,i)
    random = np.random.randint(0,len(faces)-1)
    plt.imshow(faces[random], cmap='gray')
    plt.title(labels[random])
plt.tight_layout()
plt.show()

"""- **Loading the face detector model (MTCNN) & the pre trained FaceNet model :**

---


"""

mtcnn = MTCNN(keep_all=True)
model = InceptionResnetV1(pretrained='vggface2').eval()

"""- **Creating a function for embedding generation :**


---


"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

def get_embedding(img):

    if len(img.shape) == 2 or img.shape[-1] == 1:
        img_rgb = np.stack([img] * 3, axis=-1)
    else:
        img_rgb = img


    img_rgb = Image.fromarray((img_rgb * 255).astype(np.uint8))


    plt.imshow(img_rgb)
    plt.title("Converted to RGB")
    plt.axis('off')
    plt.show()

    # Detect faces using MTCNN
    faces, probs = mtcnn(img_rgb, return_prob=True)

    # If a face is detected, calculate embeddings
    if faces is not None and len(faces) > 0:
        face = faces[0]  # Consider the first face detected
        embedding = model(face.unsqueeze(0))  # Get embedding for the face
        embedding = embedding.detach().numpy()

        # Display the face and its embedding
        plt.imshow(face.permute(1, 2, 0).int())
        plt.title("Detected Face")
        plt.axis('off')
        plt.show()

        print("Embedding Vector:", embedding)
        return embedding
    else:
        print("No face detected.")
        return None

img = data.images[1]

# Normalize the image between 0 and 1
img_normalized = img / 255.0

#
embedding = get_embedding(img_normalized)

"""- **Generate embedding for all the images on the dataset :**


---


"""

embeddings = []
names = []
final_faces = []

for i, face in enumerate(faces):
    embedding = get_embedding(face)
    if embedding is not None:
        embeddings.append(embedding)
        final_faces.append(face)
        names.append(labels[i])

embeddings = np.vstack(embeddings)
names = np.array(names)

print(f"Embeddings shape: {embeddings.shape}")
print(f"Number of labels (names): {names.shape[0]}")
print(f"Total number of faces: {len(final_faces)}")

"""- **Creating an Index using Faiss library for Euclidean distance search :**

---


"""

embeddings_matrix = np.array(embeddings).astype('float32')

d = embeddings_matrix.shape[1]

index = faiss.IndexFlatL2(d)

index.add(embeddings_matrix)

print(f"Total embeddings stored: {index.ntotal}")

"""- **Creation a function to search for a similar face :**

---


"""

def search_similar_faces(query_embedding, k=1):
    query_embedding = query_embedding.astype('float32').reshape(1, -1)
    distances, indices = index.search(query_embedding, k)
    return distances, indices

query_image_index = 350
query_embedding = embeddings[query_image_index]

distances, indices = search_similar_faces(query_embedding)

print(f"Search results for the image at index {query_image_index}:")
print(f"Indices of nearest neighbors: {indices}")
print(f"Distances to nearest neighbors: {distances}")

def display_similar_images(indices, data):
    plt.figure(figsize=(15, 5))

    plt.subplot(1, len(indices[0]) + 1, 1)
    plt.title(f"Query Image (index {query_image_index})")

    for i, idx in enumerate(indices[0]):
        plt.subplot(1, len(indices[0]) + 1, i + 2)
        plt.imshow(data.images[idx], cmap='gray')
        plt.title(f"Neighbor {i+1} (index {idx})")

    plt.show()

display_similar_images(indices, data)

"""- **Creating an UI for image upload using Gradio library :**

---
"""

def preprocess_img(img, target_size=(160, 160)):
    img_resized = cv.resize(img, target_size)
    img_normalized = (img_resized - 127.5) / 127.5
    if len(img_normalized.shape) == 2 or img_normalized.shape[-1] == 1:
        img_rgb = np.stack([img_normalized] * 3, axis=-1)
    else:
        img_rgb = img_normalized

    img_rgb = np.transpose(img_rgb, (2, 0, 1))

    return img_rgb

def recognize_face(img):
    if len(img.shape) == 2 or img.shape[-1] == 1:
        img = np.stack([img] * 3, axis=-1)
    boxes, _ = mtcnn.detect(img)
    if boxes is None or len(boxes) == 0:
        return "No face detected", None
    face_cropped = img[int(boxes[0][1]):int(boxes[0][3]), int(boxes[0][0]):int(boxes[0][2])]
    input_preprocessed = preprocess_img(face_cropped)
    input_tensor = torch.tensor([input_preprocessed], dtype=torch.float32)
    with torch.no_grad():
        new_face_embedding = model(input_tensor).cpu().numpy()[0]
    return "Face recognized", face_cropped

def face_recognition_interface(image):
    result, face_image = recognize_face(image)
    return result

interface = gr.Interface(
    fn=recognize_face,
    inputs=gr.Image(type = "numpy", label = "Upload a face image"),
    outputs = [
        gr.Image(label="The detected face from the input image"),
        gr.Image(label="The picture of the first best match"),
    ],
    title = "Face Recognition Application",
    description = "Upload a face image and find the its twin from lfw dataset ."
)

interface.launch()